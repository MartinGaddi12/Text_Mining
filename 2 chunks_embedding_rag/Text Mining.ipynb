{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4015bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Guardado de chunks en archivos JSONL divididos por partes\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# üì¶ Carga de .jsonl l√≠nea por l√≠nea usando JSONLoader (modo json_lines)\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# ‚úÇÔ∏è Definici√≥n de splitters (Character y Recursive)\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# üß† Cargar modelo de embeddings (sentence-transformers)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "import textwrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0e2f94d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cargamos cada l√≠nea del archivo como un JSON independiente (modo json_lines=True)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Extraemos solo el campo \"document_text\" de cada l√≠nea\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Devolvemos objetos Document(page_content=..., metadata={})\u001b[39;00m\n\u001b[32m      5\u001b[39m loader = JSONLoader(\n\u001b[32m      6\u001b[39m     file_path=\u001b[33m\"\u001b[39m\u001b[33m./Input/all_drugs_docs.jsonl\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     jq_schema=\u001b[33m\"\u001b[39m\u001b[33m.document_text\u001b[39m\u001b[33m\"\u001b[39m,       \u001b[38;5;66;03m# Extrae solo el contenido del texto\u001b[39;00m\n\u001b[32m      8\u001b[39m     text_content=\u001b[38;5;28;01mFalse\u001b[39;00m,               \u001b[38;5;66;03m# Nos devuelve Document(), no solo string\u001b[39;00m\n\u001b[32m      9\u001b[39m     json_lines=\u001b[38;5;28;01mTrue\u001b[39;00m                   \u001b[38;5;66;03m# üí° ¬°IMPORTANTE! Activa modo .jsonl (una l√≠nea = un JSON)\u001b[39;00m\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m documents = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìÑ Documentos cargados correctamente: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brusa\\anaconda3\\envs\\LucasBrusasca\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:32\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brusa\\anaconda3\\envs\\LucasBrusasca\\Lib\\site-packages\\langchain_community\\document_loaders\\json_loader.py:140\u001b[39m, in \u001b[36mJSONLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._json_lines:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file_path.open(encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8-sig\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m            \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:322\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cargamos cada l√≠nea del archivo como un JSON independiente (modo json_lines=True)\n",
    "# Extraemos solo el campo \"document_text\" de cada l√≠nea\n",
    "# Devolvemos objetos Document(page_content=..., metadata={})\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=\"./Input/all_drugs_docs.jsonl\",\n",
    "    jq_schema=\".document_text\",       # Extrae solo el contenido del texto\n",
    "    text_content=False,               # Nos devuelve Document(), no solo string\n",
    "    json_lines=True                   # üí° ¬°IMPORTANTE! Activa modo .jsonl (una l√≠nea = un JSON)\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(f\"üìÑ Documentos cargados correctamente: {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51406a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Splitters configurados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n global del chunking\n",
    "chunk_size = 1000      # Tama√±o de cada fragmento (en caracteres)\n",
    "chunk_overlap = 100    # Superposici√≥n entre chunks (para no cortar ideas)\n",
    "\n",
    "# Splitter 1: Corta por bloques de caracteres fijos\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Splitter 2: Intenta cortar primero por saltos de l√≠nea, luego por frases, luego por palabras\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Splitters configurados correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8aaa02eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2313, which is longer than the specified 1000\n",
      "Created a chunk of size 1286, which is longer than the specified 1000\n",
      "Created a chunk of size 1731, which is longer than the specified 1000\n",
      "Created a chunk of size 1217, which is longer than the specified 1000\n",
      "Created a chunk of size 1089, which is longer than the specified 1000\n",
      "Created a chunk of size 2376, which is longer than the specified 1000\n",
      "Created a chunk of size 1574, which is longer than the specified 1000\n",
      "Created a chunk of size 2547, which is longer than the specified 1000\n",
      "Created a chunk of size 1987, which is longer than the specified 1000\n",
      "Created a chunk of size 3582, which is longer than the specified 1000\n",
      "Created a chunk of size 1847, which is longer than the specified 1000\n",
      "Created a chunk of size 9711, which is longer than the specified 1000\n",
      "Created a chunk of size 2317, which is longer than the specified 1000\n",
      "Created a chunk of size 1689, which is longer than the specified 1000\n",
      "Created a chunk of size 1682, which is longer than the specified 1000\n",
      "Created a chunk of size 1727, which is longer than the specified 1000\n",
      "Created a chunk of size 1398, which is longer than the specified 1000\n",
      "Created a chunk of size 7919, which is longer than the specified 1000\n",
      "Created a chunk of size 1977, which is longer than the specified 1000\n",
      "Created a chunk of size 4240, which is longer than the specified 1000\n",
      "Created a chunk of size 2345, which is longer than the specified 1000\n",
      "Created a chunk of size 1476, which is longer than the specified 1000\n",
      "Created a chunk of size 2154, which is longer than the specified 1000\n",
      "Created a chunk of size 5536, which is longer than the specified 1000\n",
      "Created a chunk of size 1555, which is longer than the specified 1000\n",
      "Created a chunk of size 1673, which is longer than the specified 1000\n",
      "Created a chunk of size 1505, which is longer than the specified 1000\n",
      "Created a chunk of size 1325, which is longer than the specified 1000\n",
      "Created a chunk of size 5562, which is longer than the specified 1000\n",
      "Created a chunk of size 5962, which is longer than the specified 1000\n",
      "Created a chunk of size 2620, which is longer than the specified 1000\n",
      "Created a chunk of size 3324, which is longer than the specified 1000\n",
      "Created a chunk of size 2573, which is longer than the specified 1000\n",
      "Created a chunk of size 14251, which is longer than the specified 1000\n",
      "Created a chunk of size 2753, which is longer than the specified 1000\n",
      "Created a chunk of size 1118, which is longer than the specified 1000\n",
      "Created a chunk of size 19080, which is longer than the specified 1000\n",
      "Created a chunk of size 1798, which is longer than the specified 1000\n",
      "Created a chunk of size 1589, which is longer than the specified 1000\n",
      "Created a chunk of size 4814, which is longer than the specified 1000\n",
      "Created a chunk of size 4692, which is longer than the specified 1000\n",
      "Created a chunk of size 1562, which is longer than the specified 1000\n",
      "Created a chunk of size 1568, which is longer than the specified 1000\n",
      "Created a chunk of size 4683, which is longer than the specified 1000\n",
      "Created a chunk of size 2885, which is longer than the specified 1000\n",
      "Created a chunk of size 1882, which is longer than the specified 1000\n",
      "Created a chunk of size 6709, which is longer than the specified 1000\n",
      "Created a chunk of size 2279, which is longer than the specified 1000\n",
      "Created a chunk of size 7237, which is longer than the specified 1000\n",
      "Created a chunk of size 2459, which is longer than the specified 1000\n",
      "Created a chunk of size 3072, which is longer than the specified 1000\n",
      "Created a chunk of size 1321, which is longer than the specified 1000\n",
      "Created a chunk of size 10554, which is longer than the specified 1000\n",
      "Created a chunk of size 3040, which is longer than the specified 1000\n",
      "Created a chunk of size 1783, which is longer than the specified 1000\n",
      "Created a chunk of size 2370, which is longer than the specified 1000\n",
      "Created a chunk of size 6007, which is longer than the specified 1000\n",
      "Created a chunk of size 1994, which is longer than the specified 1000\n",
      "Created a chunk of size 9153, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ CharacterTextSplitter gener√≥ 221652 chunks\n",
      "üî∏ RecursiveCharacterTextSplitter gener√≥ 452732 chunks\n"
     ]
    }
   ],
   "source": [
    "# üîÅ Aplicaci√≥n de los splitters\n",
    "\n",
    "# Aplicamos el CharacterTextSplitter a todos los documentos\n",
    "char_chunks = char_splitter.split_documents(documents)\n",
    "print(f\"üîπ CharacterTextSplitter gener√≥ {len(char_chunks)} chunks\")\n",
    "\n",
    "# Aplicamos el RecursiveCharacterTextSplitter a todos los documentos\n",
    "recursive_chunks = recursive_splitter.split_documents(documents)\n",
    "print(f\"üî∏ RecursiveCharacterTextSplitter gener√≥ {len(recursive_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11f36baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Guardado: ./Output/chunks_char_1000_part1.jsonl (50000 chunks)\n",
      "‚úÖ Guardado: ./Output/chunks_char_1000_part2.jsonl (50000 chunks)\n",
      "‚úÖ Guardado: ./Output/chunks_char_1000_part3.jsonl (50000 chunks)\n",
      "‚úÖ Guardado: ./Output/chunks_char_1000_part4.jsonl (50000 chunks)\n",
      "‚úÖ Guardado: ./Output/chunks_char_1000_part5.jsonl (21652 chunks)\n",
      "‚úÖ Guardado: ./Output/chunks_recursive_1000_part1.jsonl (50000 chunks)\n",
      "‚úÖ Guardado: ./Output/chunks_recursive_1000_part2.jsonl (50000 chunks)\n",
      "‚úÖ Guardado: ./Output/chunks_recursive_1000_part3.jsonl (50000 chunks)\n",
      "‚úÖ Guardado: ./Output/chunks_recursive_1000_part4.jsonl (50000 chunks)\n",
      "‚úÖ Guardado: ./Output/chunks_recursive_1000_part5.jsonl (50000 chunks)\n",
      "‚úÖ Guardado: ./Output/chunks_recursive_1000_part6.jsonl (50000 chunks)\n",
      "‚úÖ Guardado: ./Output/chunks_recursive_1000_part7.jsonl (50000 chunks)\n",
      "‚úÖ Guardado: ./Output/chunks_recursive_1000_part8.jsonl (50000 chunks)\n",
      "‚úÖ Guardado: ./Output/chunks_recursive_1000_part9.jsonl (50000 chunks)\n",
      "‚úÖ Guardado: ./Output/chunks_recursive_1000_part10.jsonl (2732 chunks)\n"
     ]
    }
   ],
   "source": [
    "# Crear carpeta de salida si no existe\n",
    "os.makedirs(\"./Output\", exist_ok=True)\n",
    "\n",
    "# Funci√≥n para guardar chunks divididos en partes\n",
    "def save_chunks_in_parts(chunks, name_prefix, part_size=50000):\n",
    "    for i in range(0, len(chunks), part_size):\n",
    "        part = chunks[i:i+part_size]\n",
    "        file_path = f\"./Output/{name_prefix}_part{i//part_size + 1}.jsonl\"\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for doc in part:\n",
    "                json.dump({\"text\": doc.page_content}, f)\n",
    "                f.write(\"\\n\")\n",
    "        print(f\"‚úÖ Guardado: {file_path} ({len(part)} chunks)\")\n",
    "\n",
    "# Guardar resultados de ambos splitters\n",
    "save_chunks_in_parts(char_chunks, \"chunks_char_1000\")\n",
    "save_chunks_in_parts(recursive_chunks, \"chunks_recursive_1000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba9e7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo de embeddings cargado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Elegimos un modelo r√°pido, gratuito y potente para retrieval\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"‚úÖ Modelo de embeddings cargado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9befb8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Funci√≥n para procesar un archivo de chunks y generar embeddings\n",
    "\n",
    "def generar_embeddings_desde_jsonl(input_path):\n",
    "    \"\"\"\n",
    "    Carga los chunks desde un archivo .jsonl,\n",
    "    genera embeddings usando el modelo cargado,\n",
    "    y devuelve una lista de diccionarios con texto + embedding.\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            texto = obj.get(\"text\", \"\").strip()\n",
    "            if texto:  # Evita procesar l√≠neas vac√≠as\n",
    "                vector = model.encode(texto).tolist()\n",
    "                resultados.append({\n",
    "                    \"text\": texto,\n",
    "                    \"embedding\": vector\n",
    "                })\n",
    "    print(f\"üì¶ Embeddings generados desde: {input_path} ‚Üí {len(resultados)} chunks\")\n",
    "    return resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55aa330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Guardado de embeddings en partes (.jsonl)\n",
    "\n",
    "def guardar_embeddings_por_partes(embeddings, name_prefix, part_size=50000):\n",
    "    os.makedirs(\"./Embeddings\", exist_ok=True)\n",
    "    for i in range(0, len(embeddings), part_size):\n",
    "        parte = embeddings[i:i + part_size]\n",
    "        ruta = f\"./Embeddings/{name_prefix}_part{i//part_size + 1}.jsonl\"\n",
    "        with open(ruta, \"w\", encoding=\"utf-8\") as f:\n",
    "            for item in parte:\n",
    "                json.dump(item, f)\n",
    "                f.write(\"\\n\")\n",
    "        print(f\"‚úÖ Guardado: {ruta} ({len(parte)} vectores)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß± Procesar chunks_recursive_1000_* desde os.listdir()\n",
    "\n",
    "# Crear carpeta Embeddings si no existe\n",
    "os.makedirs(\"./Embeddings\", exist_ok=True)\n",
    "\n",
    "# Buscar todos los archivos en /Output que empiecen con chunks_recursive_1000_part\n",
    "archivos = [\n",
    "    f for f in os.listdir(\"./Output\") \n",
    "    if f.startswith(\"chunks_recursive_1000_part\") and f.endswith(\".jsonl\")\n",
    "]\n",
    "\n",
    "# Ordenarlos naturalmente por n√∫mero de parte\n",
    "archivos_ordenados = sorted(\n",
    "    archivos,\n",
    "    key=lambda x: int(x.replace(\"chunks_recursive_1000_part\", \"\").replace(\".jsonl\", \"\"))\n",
    ")\n",
    "\n",
    "# Procesar uno por uno\n",
    "for archivo in archivos_ordenados:\n",
    "    ruta_entrada = os.path.join(\"./Output\", archivo)\n",
    "    \n",
    "    # Extraer n√∫mero de parte autom√°ticamente\n",
    "    nro_parte = archivo.replace(\"chunks_recursive_1000_part\", \"\").replace(\".jsonl\", \"\")\n",
    "    \n",
    "    # Ruta de salida\n",
    "    nombre_salida = f\"embeddings_recursive_part{nro_parte}.jsonl\"\n",
    "    ruta_salida = os.path.join(\"./Embeddings\", nombre_salida)\n",
    "    \n",
    "    # Saltar si ya existe\n",
    "    if os.path.exists(ruta_salida):\n",
    "        print(f\"‚ö†Ô∏è  Ya existe {ruta_salida}, se omite.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üöÄ Procesando {ruta_entrada} ‚Üí Parte {nro_parte}\")\n",
    "    \n",
    "    # Generar embeddings\n",
    "    embeddings = generar_embeddings_desde_jsonl(ruta_entrada)\n",
    "    \n",
    "    # Guardar embeddings\n",
    "    guardar_embeddings_por_partes(embeddings, f\"embeddings_recursive_part{nro_parte}\")\n",
    "\n",
    "    print(f\"‚úÖ Listo: Parte {nro_parte}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d0fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí† Procesar chunks_char_1000_* desde os.listdir()\n",
    "\n",
    "\n",
    "# Crear carpeta Embeddings si no existe\n",
    "os.makedirs(\"./Embeddings\", exist_ok=True)\n",
    "\n",
    "# Buscar todos los archivos en /Output que empiecen con chunks_char_1000_part\n",
    "archivos_char = [\n",
    "    f for f in os.listdir(\"./Output\") \n",
    "    if f.startswith(\"chunks_char_1000_part\") and f.endswith(\".jsonl\")\n",
    "]\n",
    "\n",
    "# Ordenarlos naturalmente por n√∫mero de parte\n",
    "archivos_char_ordenados = sorted(\n",
    "    archivos_char,\n",
    "    key=lambda x: int(x.replace(\"chunks_char_1000_part\", \"\").replace(\".jsonl\", \"\"))\n",
    ")\n",
    "\n",
    "# Procesar uno por uno\n",
    "for archivo in archivos_char_ordenados:\n",
    "    ruta_entrada = os.path.join(\"./Output\", archivo)\n",
    "    \n",
    "    # Extraer n√∫mero de parte autom√°ticamente\n",
    "    nro_parte = archivo.replace(\"chunks_char_1000_part\", \"\").replace(\".jsonl\", \"\")\n",
    "    \n",
    "    # Ruta de salida\n",
    "    nombre_salida = f\"embeddings_char_part{nro_parte}.jsonl\"\n",
    "    ruta_salida = os.path.join(\"./Embeddings\", nombre_salida)\n",
    "    \n",
    "    # Saltar si ya existe\n",
    "    if os.path.exists(ruta_salida):\n",
    "        print(f\"‚ö†Ô∏è  Ya existe {ruta_salida}, se omite.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üöÄ Procesando {ruta_entrada} ‚Üí Parte {nro_parte}\")\n",
    "    \n",
    "    # Generar embeddings\n",
    "    embeddings = generar_embeddings_desde_jsonl(ruta_entrada)\n",
    "    \n",
    "    # Guardar embeddings\n",
    "    guardar_embeddings_por_partes(embeddings, f\"embeddings_char_part{nro_parte}\")\n",
    "\n",
    "    print(f\"‚úÖ Listo: Parte {nro_parte}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f2254f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Base FAISS creada con √©xito para 'recursive'\n",
      "üìÑ Documentos indexados: 488874\n",
      "üìÅ Carpeta de salida: ./faiss_index_recursive\n"
     ]
    }
   ],
   "source": [
    "# üîß CONFIGURACI√ìN: Elige modelo 'char' o 'recursive'\n",
    "splitter = \"recursive\"  # ‚Üê Cambiar a \"recursive\" si corresponde\n",
    "\n",
    "# üìÅ Ruta base REAL (la que ven√≠s usando vos)\n",
    "carpeta = \"./Embeddings/Overlap 250/\"\n",
    "output_dir = f\"./faiss_index_{splitter}\"\n",
    "\n",
    "# üìÇ Crear carpeta de salida si no existe\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# üìÇ Listar los archivos .jsonl del splitter elegido\n",
    "archivos = sorted([\n",
    "    archivo for archivo in os.listdir(carpeta)\n",
    "    if archivo.startswith(f\"embeddings_{splitter}\") and archivo.endswith(\".jsonl\")\n",
    "])\n",
    "\n",
    "# ‚úÖ Inicializar listas para textos y embeddings\n",
    "texts = []\n",
    "embeddings = []\n",
    "dimensiones = set()\n",
    "\n",
    "# üì• Leer textos y embeddings desde cada archivo .jsonl\n",
    "for archivo in archivos:\n",
    "    ruta_completa = os.path.join(carpeta, archivo)\n",
    "    with open(ruta_completa, \"r\", encoding=\"utf-8\") as f:\n",
    "        for linea in f:\n",
    "            obj = json.loads(linea)\n",
    "            texts.append(obj[\"text\"])\n",
    "            embeddings.append(obj[\"embedding\"])\n",
    "            dimensiones.add(len(obj[\"embedding\"]))\n",
    "\n",
    "# ‚ö†Ô∏è Validar que todas las dimensiones sean iguales\n",
    "if len(dimensiones) != 1:\n",
    "    raise ValueError(f\"‚ö†Ô∏è Embeddings con dimensiones distintas detectadas: {dimensiones}\")\n",
    "\n",
    "# üîÑ Convertir embeddings a array NumPy (requisito de FAISS)\n",
    "embeddings_np = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# üß† Crear √≠ndice FAISS (FlatL2)\n",
    "index = faiss.IndexFlatL2(embeddings_np.shape[1])\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# üíæ GUARDAR EL √çNDICE VECTORIAL\n",
    "faiss.write_index(index, os.path.join(output_dir, f\"index_{splitter}.faiss\"))\n",
    "\n",
    "# üíæ GUARDAR LOS TEXTOS ASOCIADOS\n",
    "with open(os.path.join(output_dir, f\"texts_{splitter}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(texts, f)\n",
    "\n",
    "# ‚úÖ Mensaje final de confirmaci√≥n\n",
    "print(f\"‚úÖ Base FAISS creada con √©xito para '{splitter}'\")\n",
    "print(f\"üìÑ Documentos indexados: {len(texts)}\")\n",
    "print(f\"üìÅ Carpeta de salida: {output_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d2b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Cargando √≠ndice y textos guardados‚Ä¶\n",
      "üìä Mostrando los primeros 3 textos:\n",
      "Texto 1:\n",
      "'Professional Monographs Browse medications by letter: Show a list of drugs beginning with the first two letters: Aa Ab Ac Ad Ae Af Ag Ah Ai Aj Ak Al Am An Ao Ap Aq Ar As At Au Av Aw Ax Ay Az 0-9'\n",
      "\n",
      "Texto 2:\n",
      "'A/B Otic Uses for A/B Otic: Antipyrine and benzocaine combination is used in the ear to help relieve the pain, swelling, and congestion of some ear infections. It will not cure the infection itself. An antibiotic will be needed to treat the infection. This medicine is also used to soften earwax so t'\n",
      "\n",
      "Texto 3:\n",
      "'A/B Otic Before using A/B Otic: In deciding to use a medicine, the risks of taking the medicine must be weighed against the good it will do. This is a decision you and your doctor will make. For this medicine, the following should be considered: Allergies Tell your doctor if you have ever had any un'\n",
      "\n",
      "‚úÖ √çndice cargado con √©xito: 488,874 documentos.\n",
      "\n",
      "üìö Pasajes recuperados (TOP_K):\n",
      "\n",
      "‚Ä¢ Pasaje 1:\n",
      "Tacrolimus (Oral) Side Effects of tacrolimus: Along with its needed effects, a medicine may cause some unwanted effects. Although not all of these side effects may occur, if they do occur they may need medical attention. Check with your doctor immediatelyif any of the following side effects occur: More common Abnormal dreams agitation chills [...]\n",
      "\n",
      "‚Ä¢ Pasaje 2:\n",
      "Tacrolimus (Intravenous) Side Effects of tacrolimus: Along with its needed effects, a medicine may cause some unwanted effects. Although not all of these side effects may occur, if they do occur they may need medical attention. Check with your doctor or nurse immediatelyif any of the following side effects occur: More common Abnormal dreams [...]\n",
      "\n",
      "‚Ä¢ Pasaje 3:\n",
      "Tacrolimus PATIENT INFORMATION: Tacrolimus capsules, USP Read this Patient Information before you start taking tacrolimus and each time you get a refill. There may be new information. This information does not take the place of talking with your doctor about your medical condition or your treatment. What is the most important information I [...]\n",
      "\n",
      "‚Ä¢ Pasaje 4:\n",
      "occurs. Common side effects of Tasmar may include: dizziness,drowsiness; nausea, diarrhea, loss of appetite; sleep problems, increased dreaming; or muscle cramps. dizziness,drowsiness; nausea, diarrhea, loss of appetite; sleep problems, increased dreaming; or muscle cramps. This is not a complete list of side effects and others may occur. [...]\n",
      "\n",
      "‚Ä¢ Pasaje 5:\n",
      "Tacrine Side Effects of tacrine: Along with its needed effects, a medicine may cause some unwanted effects. Some side effects will have signs or symptoms that you can see or feel. Your doctor may watch for others by doing certain tests Tacrine may cause some serious side effects, including liver problems. You and your doctor should discuss [...]\n",
      "\n",
      "‚Ä¢ Pasaje 6:\n",
      "protection from the sun you should use. Do not cover the skin being treated with bandages, dressings or wraps. You can wear normal clothing. Avoid getting tacrolimus ointment in the eyes or mouth. Do not swallow tacrolimus ointment. If you do, call your doctor. What are the possible side effects of tacrolimus ointment? Please read the first [...]\n",
      "\n",
      "‚Ä¢ Pasaje 7:\n",
      "Tasimelteon Tasimelteon side effects: Get emergency medical help if you havesigns of an allergic reaction:hives; difficult breathing; swelling of your face, lips, tongue, or throat. Call your doctor at once if you have pain or burning when you urinate. Side effects may be more likely in older adults. Common side effects of tasimelteon may [...]\n",
      "\n",
      "‚Ä¢ Pasaje 8:\n",
      "Tacrolimus oral and injection Tacrolimus side effects: Get emergency medical help if you havesigns of an allergic reaction: hives; difficult breathing; swelling of your face, lips, tongue, or throat. You may get infections more easily, even serious or fatal infections.Call your doctor right away if you have signs of infection such as:fever, [...]\n",
      "\n",
      "ü§ñ Respuesta RAG (modelo local):\n",
      "Los efectos secundarios de Tacrolimus pueden ser graves y incluyen:\n",
      "\n",
      "*   Infecciones graves, incluso fatales\n",
      "*   C√°ncer, incluyendo c√°ncer de piel y linfoma\n",
      "*   Reacciones al√©rgicas, como hinchaz√≥n del rostro, labios, lengua o garganta\n",
      "\n",
      "Adem√°s de estos efectos secundarios graves, los pacientes que toman Tacrolimus pueden experimentar otros efectos secundarios, incluyendo:\n",
      "\n",
      "*   Abdominal dolor\n",
      "*   Aumento de apetito\n",
      "*   Ardor de est√≥mago\n",
      "*   Confusi√≥n\n",
      "*   Dolor en las articulaciones\n",
      "*   Fiebre\n",
      "*   Hambre aumentada\n",
      "*   Infecciones del tracto urinario\n",
      "*   Mareos\n",
      "*   N√°useas\n",
      "*   P√©rdida de apetito\n",
      "*   P√©rdida de peso\n",
      "*   P√©rdida de conciencia\n",
      "*   P√©rdida de energ√≠a o debilidad\n",
      "*   Problemas para dormir\n",
      "*   Rinitis\n",
      "*   Sensaci√≥n de malestar general\n",
      "*   Somnolencia\n",
      "*   Temblores y sacudidas de las manos\n"
     ]
    }
   ],
   "source": [
    "# üõ†Ô∏è CONFIGURACI√ìN\n",
    "\n",
    "INDEX_DIR = \"./faiss_index_recursive\"     # Ruta al √≠ndice FAISS creado\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"      # Embeddings originales\n",
    "TOP_K = 8                                 # N√∫mero de pasajes a recuperar\n",
    "LLM_MODEL = \"llama3.1\"                    # Modelo local v√≠a Ollama (llama3.1, mistral, llama3, etc.)\n",
    "\n",
    " \n",
    "# üöÄ CARGA DE LA BASE FAISS Y TEXTOS ASOCIADOS\n",
    "\n",
    "print(\"üîπ Cargando √≠ndice y textos guardados‚Ä¶\")\n",
    "index = faiss.read_index(os.path.join(INDEX_DIR, \"index_recursive.faiss\"))\n",
    "\n",
    "with open(os.path.join(INDEX_DIR, \"texts_recursive.pkl\"), \"rb\") as f:\n",
    "    textos = pickle.load(f)\n",
    "\n",
    "print(\"üìä Mostrando los primeros 3 textos:\")\n",
    "for i, t in enumerate(textos[:3]):\n",
    "    print(f\"Texto {i+1}:\\n{repr(t[:300])}\\n\")\n",
    "\n",
    "print(f\"‚úÖ √çndice cargado con √©xito: {index.ntotal:,} documentos.\")\n",
    "\n",
    "\n",
    "# üß† EMBEDDINGS PARA CONSULTAS\n",
    "\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "def embed_query(query: str) -> np.ndarray:\n",
    "    return np.array(embedder.encode(query)).astype(\"float32\")\n",
    "\n",
    "\n",
    "# üîç RECUPERACI√ìN SEM√ÅNTICA\n",
    "\n",
    "def retrieve(query: str, k: int = TOP_K) -> list[str]:\n",
    "    query_vec = embed_query(query).reshape(1, -1)\n",
    "    _, idx = index.search(query_vec, k)\n",
    "    return [textos[i] for i in idx[0]]\n",
    "\n",
    "\n",
    "# ü§ñ CONFIGURACI√ìN DEL MODELO\n",
    "\n",
    "llm = Ollama(model=LLM_MODEL, temperature=0.7, num_predict=512)\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    textwrap.dedent(\"\"\"\\\n",
    "    Sos un experto en farmacolog√≠a cl√≠nica. Basado exclusivamente en el contexto provisto, brind√° una respuesta completa y detallada que responda espec√≠ficamente a la siguiente pregunta.\n",
    "    Si hay informaci√≥n cl√≠nica relevante (mecanismo, precauciones, embarazo, interacciones, etc.), inclu√≠la.\n",
    "    No infieras nada que no est√© presente expl√≠citamente.\n",
    "\n",
    "    CONTEXTO:\n",
    "    {context}\n",
    "\n",
    "    PREGUNTA:\n",
    "    {question}\n",
    "\n",
    "    RESPUESTA BASADA √öNICAMENTE EN EL CONTEXTO:\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# üîÅ FUNCI√ìN RAG\n",
    "\n",
    "def respuesta_rag(query: str) -> str:\n",
    "    contexto = \"\\n\\n\".join(retrieve(query))\n",
    "    prompt = prompt_template.format(context=contexto, question=query)\n",
    "    respuesta = llm.invoke(prompt)\n",
    "    return respuesta\n",
    "\n",
    "\n",
    "# ‚úÖ CONSULTA MANUAL DESDE NOTEBOOK\n",
    "\n",
    "query = \"What are the side effects of tacrolimus\"\n",
    "\n",
    "\n",
    "if not query.strip():\n",
    "    print(\"‚ùó Pregunta vac√≠a. Intenta escribir algo.\")\n",
    "else:\n",
    "    print(\"\\nüìö Pasajes recuperados (TOP_K):\")\n",
    "    for idx, txt in enumerate(retrieve(query), 1):\n",
    "        print(f\"\\n‚Ä¢ Pasaje {idx}:\\n{textwrap.shorten(txt, 350)}\")\n",
    "\n",
    "    print(\"\\nü§ñ Respuesta RAG (modelo local):\")\n",
    "    print(respuesta_rag(query))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LucasBrusasca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
