{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4015bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 Guardado de chunks en archivos JSONL divididos por partes\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# 📦 Carga de .jsonl línea por línea usando JSONLoader (modo json_lines)\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# ✂️ Definición de splitters (Character y Recursive)\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# 🧠 Cargar modelo de embeddings (sentence-transformers)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "import textwrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0e2f94d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cargamos cada línea del archivo como un JSON independiente (modo json_lines=True)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Extraemos solo el campo \"document_text\" de cada línea\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Devolvemos objetos Document(page_content=..., metadata={})\u001b[39;00m\n\u001b[32m      5\u001b[39m loader = JSONLoader(\n\u001b[32m      6\u001b[39m     file_path=\u001b[33m\"\u001b[39m\u001b[33m./Input/all_drugs_docs.jsonl\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     jq_schema=\u001b[33m\"\u001b[39m\u001b[33m.document_text\u001b[39m\u001b[33m\"\u001b[39m,       \u001b[38;5;66;03m# Extrae solo el contenido del texto\u001b[39;00m\n\u001b[32m      8\u001b[39m     text_content=\u001b[38;5;28;01mFalse\u001b[39;00m,               \u001b[38;5;66;03m# Nos devuelve Document(), no solo string\u001b[39;00m\n\u001b[32m      9\u001b[39m     json_lines=\u001b[38;5;28;01mTrue\u001b[39;00m                   \u001b[38;5;66;03m# 💡 ¡IMPORTANTE! Activa modo .jsonl (una línea = un JSON)\u001b[39;00m\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m documents = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m📄 Documentos cargados correctamente: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brusa\\anaconda3\\envs\\LucasBrusasca\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:32\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brusa\\anaconda3\\envs\\LucasBrusasca\\Lib\\site-packages\\langchain_community\\document_loaders\\json_loader.py:140\u001b[39m, in \u001b[36mJSONLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._json_lines:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file_path.open(encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8-sig\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m            \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:322\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cargamos cada línea del archivo como un JSON independiente (modo json_lines=True)\n",
    "# Extraemos solo el campo \"document_text\" de cada línea\n",
    "# Devolvemos objetos Document(page_content=..., metadata={})\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=\"./Input/all_drugs_docs.jsonl\",\n",
    "    jq_schema=\".document_text\",       # Extrae solo el contenido del texto\n",
    "    text_content=False,               # Nos devuelve Document(), no solo string\n",
    "    json_lines=True                   # 💡 ¡IMPORTANTE! Activa modo .jsonl (una línea = un JSON)\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(f\"📄 Documentos cargados correctamente: {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51406a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Splitters configurados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Configuración global del chunking\n",
    "chunk_size = 1000      # Tamaño de cada fragmento (en caracteres)\n",
    "chunk_overlap = 100    # Superposición entre chunks (para no cortar ideas)\n",
    "\n",
    "# Splitter 1: Corta por bloques de caracteres fijos\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Splitter 2: Intenta cortar primero por saltos de línea, luego por frases, luego por palabras\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "print(\"✅ Splitters configurados correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8aaa02eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2313, which is longer than the specified 1000\n",
      "Created a chunk of size 1286, which is longer than the specified 1000\n",
      "Created a chunk of size 1731, which is longer than the specified 1000\n",
      "Created a chunk of size 1217, which is longer than the specified 1000\n",
      "Created a chunk of size 1089, which is longer than the specified 1000\n",
      "Created a chunk of size 2376, which is longer than the specified 1000\n",
      "Created a chunk of size 1574, which is longer than the specified 1000\n",
      "Created a chunk of size 2547, which is longer than the specified 1000\n",
      "Created a chunk of size 1987, which is longer than the specified 1000\n",
      "Created a chunk of size 3582, which is longer than the specified 1000\n",
      "Created a chunk of size 1847, which is longer than the specified 1000\n",
      "Created a chunk of size 9711, which is longer than the specified 1000\n",
      "Created a chunk of size 2317, which is longer than the specified 1000\n",
      "Created a chunk of size 1689, which is longer than the specified 1000\n",
      "Created a chunk of size 1682, which is longer than the specified 1000\n",
      "Created a chunk of size 1727, which is longer than the specified 1000\n",
      "Created a chunk of size 1398, which is longer than the specified 1000\n",
      "Created a chunk of size 7919, which is longer than the specified 1000\n",
      "Created a chunk of size 1977, which is longer than the specified 1000\n",
      "Created a chunk of size 4240, which is longer than the specified 1000\n",
      "Created a chunk of size 2345, which is longer than the specified 1000\n",
      "Created a chunk of size 1476, which is longer than the specified 1000\n",
      "Created a chunk of size 2154, which is longer than the specified 1000\n",
      "Created a chunk of size 5536, which is longer than the specified 1000\n",
      "Created a chunk of size 1555, which is longer than the specified 1000\n",
      "Created a chunk of size 1673, which is longer than the specified 1000\n",
      "Created a chunk of size 1505, which is longer than the specified 1000\n",
      "Created a chunk of size 1325, which is longer than the specified 1000\n",
      "Created a chunk of size 5562, which is longer than the specified 1000\n",
      "Created a chunk of size 5962, which is longer than the specified 1000\n",
      "Created a chunk of size 2620, which is longer than the specified 1000\n",
      "Created a chunk of size 3324, which is longer than the specified 1000\n",
      "Created a chunk of size 2573, which is longer than the specified 1000\n",
      "Created a chunk of size 14251, which is longer than the specified 1000\n",
      "Created a chunk of size 2753, which is longer than the specified 1000\n",
      "Created a chunk of size 1118, which is longer than the specified 1000\n",
      "Created a chunk of size 19080, which is longer than the specified 1000\n",
      "Created a chunk of size 1798, which is longer than the specified 1000\n",
      "Created a chunk of size 1589, which is longer than the specified 1000\n",
      "Created a chunk of size 4814, which is longer than the specified 1000\n",
      "Created a chunk of size 4692, which is longer than the specified 1000\n",
      "Created a chunk of size 1562, which is longer than the specified 1000\n",
      "Created a chunk of size 1568, which is longer than the specified 1000\n",
      "Created a chunk of size 4683, which is longer than the specified 1000\n",
      "Created a chunk of size 2885, which is longer than the specified 1000\n",
      "Created a chunk of size 1882, which is longer than the specified 1000\n",
      "Created a chunk of size 6709, which is longer than the specified 1000\n",
      "Created a chunk of size 2279, which is longer than the specified 1000\n",
      "Created a chunk of size 7237, which is longer than the specified 1000\n",
      "Created a chunk of size 2459, which is longer than the specified 1000\n",
      "Created a chunk of size 3072, which is longer than the specified 1000\n",
      "Created a chunk of size 1321, which is longer than the specified 1000\n",
      "Created a chunk of size 10554, which is longer than the specified 1000\n",
      "Created a chunk of size 3040, which is longer than the specified 1000\n",
      "Created a chunk of size 1783, which is longer than the specified 1000\n",
      "Created a chunk of size 2370, which is longer than the specified 1000\n",
      "Created a chunk of size 6007, which is longer than the specified 1000\n",
      "Created a chunk of size 1994, which is longer than the specified 1000\n",
      "Created a chunk of size 9153, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 CharacterTextSplitter generó 221652 chunks\n",
      "🔸 RecursiveCharacterTextSplitter generó 452732 chunks\n"
     ]
    }
   ],
   "source": [
    "# 🔁 Aplicación de los splitters\n",
    "\n",
    "# Aplicamos el CharacterTextSplitter a todos los documentos\n",
    "char_chunks = char_splitter.split_documents(documents)\n",
    "print(f\"🔹 CharacterTextSplitter generó {len(char_chunks)} chunks\")\n",
    "\n",
    "# Aplicamos el RecursiveCharacterTextSplitter a todos los documentos\n",
    "recursive_chunks = recursive_splitter.split_documents(documents)\n",
    "print(f\"🔸 RecursiveCharacterTextSplitter generó {len(recursive_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11f36baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Guardado: ./Output/chunks_char_1000_part1.jsonl (50000 chunks)\n",
      "✅ Guardado: ./Output/chunks_char_1000_part2.jsonl (50000 chunks)\n",
      "✅ Guardado: ./Output/chunks_char_1000_part3.jsonl (50000 chunks)\n",
      "✅ Guardado: ./Output/chunks_char_1000_part4.jsonl (50000 chunks)\n",
      "✅ Guardado: ./Output/chunks_char_1000_part5.jsonl (21652 chunks)\n",
      "✅ Guardado: ./Output/chunks_recursive_1000_part1.jsonl (50000 chunks)\n",
      "✅ Guardado: ./Output/chunks_recursive_1000_part2.jsonl (50000 chunks)\n",
      "✅ Guardado: ./Output/chunks_recursive_1000_part3.jsonl (50000 chunks)\n",
      "✅ Guardado: ./Output/chunks_recursive_1000_part4.jsonl (50000 chunks)\n",
      "✅ Guardado: ./Output/chunks_recursive_1000_part5.jsonl (50000 chunks)\n",
      "✅ Guardado: ./Output/chunks_recursive_1000_part6.jsonl (50000 chunks)\n",
      "✅ Guardado: ./Output/chunks_recursive_1000_part7.jsonl (50000 chunks)\n",
      "✅ Guardado: ./Output/chunks_recursive_1000_part8.jsonl (50000 chunks)\n",
      "✅ Guardado: ./Output/chunks_recursive_1000_part9.jsonl (50000 chunks)\n",
      "✅ Guardado: ./Output/chunks_recursive_1000_part10.jsonl (2732 chunks)\n"
     ]
    }
   ],
   "source": [
    "# Crear carpeta de salida si no existe\n",
    "os.makedirs(\"./Output\", exist_ok=True)\n",
    "\n",
    "# Función para guardar chunks divididos en partes\n",
    "def save_chunks_in_parts(chunks, name_prefix, part_size=50000):\n",
    "    for i in range(0, len(chunks), part_size):\n",
    "        part = chunks[i:i+part_size]\n",
    "        file_path = f\"./Output/{name_prefix}_part{i//part_size + 1}.jsonl\"\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for doc in part:\n",
    "                json.dump({\"text\": doc.page_content}, f)\n",
    "                f.write(\"\\n\")\n",
    "        print(f\"✅ Guardado: {file_path} ({len(part)} chunks)\")\n",
    "\n",
    "# Guardar resultados de ambos splitters\n",
    "save_chunks_in_parts(char_chunks, \"chunks_char_1000\")\n",
    "save_chunks_in_parts(recursive_chunks, \"chunks_recursive_1000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba9e7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo de embeddings cargado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Elegimos un modelo rápido, gratuito y potente para retrieval\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"✅ Modelo de embeddings cargado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9befb8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Función para procesar un archivo de chunks y generar embeddings\n",
    "\n",
    "def generar_embeddings_desde_jsonl(input_path):\n",
    "    \"\"\"\n",
    "    Carga los chunks desde un archivo .jsonl,\n",
    "    genera embeddings usando el modelo cargado,\n",
    "    y devuelve una lista de diccionarios con texto + embedding.\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            texto = obj.get(\"text\", \"\").strip()\n",
    "            if texto:  # Evita procesar líneas vacías\n",
    "                vector = model.encode(texto).tolist()\n",
    "                resultados.append({\n",
    "                    \"text\": texto,\n",
    "                    \"embedding\": vector\n",
    "                })\n",
    "    print(f\"📦 Embeddings generados desde: {input_path} → {len(resultados)} chunks\")\n",
    "    return resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55aa330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 Guardado de embeddings en partes (.jsonl)\n",
    "\n",
    "def guardar_embeddings_por_partes(embeddings, name_prefix, part_size=50000):\n",
    "    os.makedirs(\"./Embeddings\", exist_ok=True)\n",
    "    for i in range(0, len(embeddings), part_size):\n",
    "        parte = embeddings[i:i + part_size]\n",
    "        ruta = f\"./Embeddings/{name_prefix}_part{i//part_size + 1}.jsonl\"\n",
    "        with open(ruta, \"w\", encoding=\"utf-8\") as f:\n",
    "            for item in parte:\n",
    "                json.dump(item, f)\n",
    "                f.write(\"\\n\")\n",
    "        print(f\"✅ Guardado: {ruta} ({len(parte)} vectores)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧱 Procesar chunks_recursive_1000_* desde os.listdir()\n",
    "\n",
    "# Crear carpeta Embeddings si no existe\n",
    "os.makedirs(\"./Embeddings\", exist_ok=True)\n",
    "\n",
    "# Buscar todos los archivos en /Output que empiecen con chunks_recursive_1000_part\n",
    "archivos = [\n",
    "    f for f in os.listdir(\"./Output\") \n",
    "    if f.startswith(\"chunks_recursive_1000_part\") and f.endswith(\".jsonl\")\n",
    "]\n",
    "\n",
    "# Ordenarlos naturalmente por número de parte\n",
    "archivos_ordenados = sorted(\n",
    "    archivos,\n",
    "    key=lambda x: int(x.replace(\"chunks_recursive_1000_part\", \"\").replace(\".jsonl\", \"\"))\n",
    ")\n",
    "\n",
    "# Procesar uno por uno\n",
    "for archivo in archivos_ordenados:\n",
    "    ruta_entrada = os.path.join(\"./Output\", archivo)\n",
    "    \n",
    "    # Extraer número de parte automáticamente\n",
    "    nro_parte = archivo.replace(\"chunks_recursive_1000_part\", \"\").replace(\".jsonl\", \"\")\n",
    "    \n",
    "    # Ruta de salida\n",
    "    nombre_salida = f\"embeddings_recursive_part{nro_parte}.jsonl\"\n",
    "    ruta_salida = os.path.join(\"./Embeddings\", nombre_salida)\n",
    "    \n",
    "    # Saltar si ya existe\n",
    "    if os.path.exists(ruta_salida):\n",
    "        print(f\"⚠️  Ya existe {ruta_salida}, se omite.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"🚀 Procesando {ruta_entrada} → Parte {nro_parte}\")\n",
    "    \n",
    "    # Generar embeddings\n",
    "    embeddings = generar_embeddings_desde_jsonl(ruta_entrada)\n",
    "    \n",
    "    # Guardar embeddings\n",
    "    guardar_embeddings_por_partes(embeddings, f\"embeddings_recursive_part{nro_parte}\")\n",
    "\n",
    "    print(f\"✅ Listo: Parte {nro_parte}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d0fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💠 Procesar chunks_char_1000_* desde os.listdir()\n",
    "\n",
    "\n",
    "# Crear carpeta Embeddings si no existe\n",
    "os.makedirs(\"./Embeddings\", exist_ok=True)\n",
    "\n",
    "# Buscar todos los archivos en /Output que empiecen con chunks_char_1000_part\n",
    "archivos_char = [\n",
    "    f for f in os.listdir(\"./Output\") \n",
    "    if f.startswith(\"chunks_char_1000_part\") and f.endswith(\".jsonl\")\n",
    "]\n",
    "\n",
    "# Ordenarlos naturalmente por número de parte\n",
    "archivos_char_ordenados = sorted(\n",
    "    archivos_char,\n",
    "    key=lambda x: int(x.replace(\"chunks_char_1000_part\", \"\").replace(\".jsonl\", \"\"))\n",
    ")\n",
    "\n",
    "# Procesar uno por uno\n",
    "for archivo in archivos_char_ordenados:\n",
    "    ruta_entrada = os.path.join(\"./Output\", archivo)\n",
    "    \n",
    "    # Extraer número de parte automáticamente\n",
    "    nro_parte = archivo.replace(\"chunks_char_1000_part\", \"\").replace(\".jsonl\", \"\")\n",
    "    \n",
    "    # Ruta de salida\n",
    "    nombre_salida = f\"embeddings_char_part{nro_parte}.jsonl\"\n",
    "    ruta_salida = os.path.join(\"./Embeddings\", nombre_salida)\n",
    "    \n",
    "    # Saltar si ya existe\n",
    "    if os.path.exists(ruta_salida):\n",
    "        print(f\"⚠️  Ya existe {ruta_salida}, se omite.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"🚀 Procesando {ruta_entrada} → Parte {nro_parte}\")\n",
    "    \n",
    "    # Generar embeddings\n",
    "    embeddings = generar_embeddings_desde_jsonl(ruta_entrada)\n",
    "    \n",
    "    # Guardar embeddings\n",
    "    guardar_embeddings_por_partes(embeddings, f\"embeddings_char_part{nro_parte}\")\n",
    "\n",
    "    print(f\"✅ Listo: Parte {nro_parte}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f2254f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Base FAISS creada con éxito para 'recursive'\n",
      "📄 Documentos indexados: 488874\n",
      "📁 Carpeta de salida: ./faiss_index_recursive\n"
     ]
    }
   ],
   "source": [
    "# 🔧 CONFIGURACIÓN: Elige modelo 'char' o 'recursive'\n",
    "splitter = \"recursive\"  # ← Cambiar a \"recursive\" si corresponde\n",
    "\n",
    "# 📁 Ruta base REAL (la que venís usando vos)\n",
    "carpeta = \"./Embeddings/Overlap 250/\"\n",
    "output_dir = f\"./faiss_index_{splitter}\"\n",
    "\n",
    "# 📂 Crear carpeta de salida si no existe\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 📂 Listar los archivos .jsonl del splitter elegido\n",
    "archivos = sorted([\n",
    "    archivo for archivo in os.listdir(carpeta)\n",
    "    if archivo.startswith(f\"embeddings_{splitter}\") and archivo.endswith(\".jsonl\")\n",
    "])\n",
    "\n",
    "# ✅ Inicializar listas para textos y embeddings\n",
    "texts = []\n",
    "embeddings = []\n",
    "dimensiones = set()\n",
    "\n",
    "# 📥 Leer textos y embeddings desde cada archivo .jsonl\n",
    "for archivo in archivos:\n",
    "    ruta_completa = os.path.join(carpeta, archivo)\n",
    "    with open(ruta_completa, \"r\", encoding=\"utf-8\") as f:\n",
    "        for linea in f:\n",
    "            obj = json.loads(linea)\n",
    "            texts.append(obj[\"text\"])\n",
    "            embeddings.append(obj[\"embedding\"])\n",
    "            dimensiones.add(len(obj[\"embedding\"]))\n",
    "\n",
    "# ⚠️ Validar que todas las dimensiones sean iguales\n",
    "if len(dimensiones) != 1:\n",
    "    raise ValueError(f\"⚠️ Embeddings con dimensiones distintas detectadas: {dimensiones}\")\n",
    "\n",
    "# 🔄 Convertir embeddings a array NumPy (requisito de FAISS)\n",
    "embeddings_np = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# 🧠 Crear índice FAISS (FlatL2)\n",
    "index = faiss.IndexFlatL2(embeddings_np.shape[1])\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# 💾 GUARDAR EL ÍNDICE VECTORIAL\n",
    "faiss.write_index(index, os.path.join(output_dir, f\"index_{splitter}.faiss\"))\n",
    "\n",
    "# 💾 GUARDAR LOS TEXTOS ASOCIADOS\n",
    "with open(os.path.join(output_dir, f\"texts_{splitter}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(texts, f)\n",
    "\n",
    "# ✅ Mensaje final de confirmación\n",
    "print(f\"✅ Base FAISS creada con éxito para '{splitter}'\")\n",
    "print(f\"📄 Documentos indexados: {len(texts)}\")\n",
    "print(f\"📁 Carpeta de salida: {output_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d2b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Cargando índice y textos guardados…\n",
      "📊 Mostrando los primeros 3 textos:\n",
      "Texto 1:\n",
      "'Professional Monographs Browse medications by letter: Show a list of drugs beginning with the first two letters: Aa Ab Ac Ad Ae Af Ag Ah Ai Aj Ak Al Am An Ao Ap Aq Ar As At Au Av Aw Ax Ay Az 0-9'\n",
      "\n",
      "Texto 2:\n",
      "'A/B Otic Uses for A/B Otic: Antipyrine and benzocaine combination is used in the ear to help relieve the pain, swelling, and congestion of some ear infections. It will not cure the infection itself. An antibiotic will be needed to treat the infection. This medicine is also used to soften earwax so t'\n",
      "\n",
      "Texto 3:\n",
      "'A/B Otic Before using A/B Otic: In deciding to use a medicine, the risks of taking the medicine must be weighed against the good it will do. This is a decision you and your doctor will make. For this medicine, the following should be considered: Allergies Tell your doctor if you have ever had any un'\n",
      "\n",
      "✅ Índice cargado con éxito: 488,874 documentos.\n",
      "\n",
      "📚 Pasajes recuperados (TOP_K):\n",
      "\n",
      "• Pasaje 1:\n",
      "Tacrolimus (Oral) Side Effects of tacrolimus: Along with its needed effects, a medicine may cause some unwanted effects. Although not all of these side effects may occur, if they do occur they may need medical attention. Check with your doctor immediatelyif any of the following side effects occur: More common Abnormal dreams agitation chills [...]\n",
      "\n",
      "• Pasaje 2:\n",
      "Tacrolimus (Intravenous) Side Effects of tacrolimus: Along with its needed effects, a medicine may cause some unwanted effects. Although not all of these side effects may occur, if they do occur they may need medical attention. Check with your doctor or nurse immediatelyif any of the following side effects occur: More common Abnormal dreams [...]\n",
      "\n",
      "• Pasaje 3:\n",
      "Tacrolimus PATIENT INFORMATION: Tacrolimus capsules, USP Read this Patient Information before you start taking tacrolimus and each time you get a refill. There may be new information. This information does not take the place of talking with your doctor about your medical condition or your treatment. What is the most important information I [...]\n",
      "\n",
      "• Pasaje 4:\n",
      "occurs. Common side effects of Tasmar may include: dizziness,drowsiness; nausea, diarrhea, loss of appetite; sleep problems, increased dreaming; or muscle cramps. dizziness,drowsiness; nausea, diarrhea, loss of appetite; sleep problems, increased dreaming; or muscle cramps. This is not a complete list of side effects and others may occur. [...]\n",
      "\n",
      "• Pasaje 5:\n",
      "Tacrine Side Effects of tacrine: Along with its needed effects, a medicine may cause some unwanted effects. Some side effects will have signs or symptoms that you can see or feel. Your doctor may watch for others by doing certain tests Tacrine may cause some serious side effects, including liver problems. You and your doctor should discuss [...]\n",
      "\n",
      "• Pasaje 6:\n",
      "protection from the sun you should use. Do not cover the skin being treated with bandages, dressings or wraps. You can wear normal clothing. Avoid getting tacrolimus ointment in the eyes or mouth. Do not swallow tacrolimus ointment. If you do, call your doctor. What are the possible side effects of tacrolimus ointment? Please read the first [...]\n",
      "\n",
      "• Pasaje 7:\n",
      "Tasimelteon Tasimelteon side effects: Get emergency medical help if you havesigns of an allergic reaction:hives; difficult breathing; swelling of your face, lips, tongue, or throat. Call your doctor at once if you have pain or burning when you urinate. Side effects may be more likely in older adults. Common side effects of tasimelteon may [...]\n",
      "\n",
      "• Pasaje 8:\n",
      "Tacrolimus oral and injection Tacrolimus side effects: Get emergency medical help if you havesigns of an allergic reaction: hives; difficult breathing; swelling of your face, lips, tongue, or throat. You may get infections more easily, even serious or fatal infections.Call your doctor right away if you have signs of infection such as:fever, [...]\n",
      "\n",
      "🤖 Respuesta RAG (modelo local):\n",
      "Los efectos secundarios de Tacrolimus pueden ser graves y incluyen:\n",
      "\n",
      "*   Infecciones graves, incluso fatales\n",
      "*   Cáncer, incluyendo cáncer de piel y linfoma\n",
      "*   Reacciones alérgicas, como hinchazón del rostro, labios, lengua o garganta\n",
      "\n",
      "Además de estos efectos secundarios graves, los pacientes que toman Tacrolimus pueden experimentar otros efectos secundarios, incluyendo:\n",
      "\n",
      "*   Abdominal dolor\n",
      "*   Aumento de apetito\n",
      "*   Ardor de estómago\n",
      "*   Confusión\n",
      "*   Dolor en las articulaciones\n",
      "*   Fiebre\n",
      "*   Hambre aumentada\n",
      "*   Infecciones del tracto urinario\n",
      "*   Mareos\n",
      "*   Náuseas\n",
      "*   Pérdida de apetito\n",
      "*   Pérdida de peso\n",
      "*   Pérdida de conciencia\n",
      "*   Pérdida de energía o debilidad\n",
      "*   Problemas para dormir\n",
      "*   Rinitis\n",
      "*   Sensación de malestar general\n",
      "*   Somnolencia\n",
      "*   Temblores y sacudidas de las manos\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ CONFIGURACIÓN\n",
    "\n",
    "INDEX_DIR = \"./faiss_index_recursive\"     # Ruta al índice FAISS creado\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"      # Embeddings originales\n",
    "TOP_K = 8                                 # Número de pasajes a recuperar\n",
    "LLM_MODEL = \"llama3.1\"                    # Modelo local vía Ollama (llama3.1, mistral, llama3, etc.)\n",
    "\n",
    " \n",
    "# 🚀 CARGA DE LA BASE FAISS Y TEXTOS ASOCIADOS\n",
    "\n",
    "print(\"🔹 Cargando índice y textos guardados…\")\n",
    "index = faiss.read_index(os.path.join(INDEX_DIR, \"index_recursive.faiss\"))\n",
    "\n",
    "with open(os.path.join(INDEX_DIR, \"texts_recursive.pkl\"), \"rb\") as f:\n",
    "    textos = pickle.load(f)\n",
    "\n",
    "print(\"📊 Mostrando los primeros 3 textos:\")\n",
    "for i, t in enumerate(textos[:3]):\n",
    "    print(f\"Texto {i+1}:\\n{repr(t[:300])}\\n\")\n",
    "\n",
    "print(f\"✅ Índice cargado con éxito: {index.ntotal:,} documentos.\")\n",
    "\n",
    "\n",
    "# 🧠 EMBEDDINGS PARA CONSULTAS\n",
    "\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "def embed_query(query: str) -> np.ndarray:\n",
    "    return np.array(embedder.encode(query)).astype(\"float32\")\n",
    "\n",
    "\n",
    "# 🔍 RECUPERACIÓN SEMÁNTICA\n",
    "\n",
    "def retrieve(query: str, k: int = TOP_K) -> list[str]:\n",
    "    query_vec = embed_query(query).reshape(1, -1)\n",
    "    _, idx = index.search(query_vec, k)\n",
    "    return [textos[i] for i in idx[0]]\n",
    "\n",
    "\n",
    "# 🤖 CONFIGURACIÓN DEL MODELO\n",
    "\n",
    "llm = Ollama(model=LLM_MODEL, temperature=0.7, num_predict=512)\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    textwrap.dedent(\"\"\"\\\n",
    "    Sos un experto en farmacología clínica. Basado exclusivamente en el contexto provisto, brindá una respuesta completa y detallada que responda específicamente a la siguiente pregunta.\n",
    "    Si hay información clínica relevante (mecanismo, precauciones, embarazo, interacciones, etc.), incluíla.\n",
    "    No infieras nada que no esté presente explícitamente.\n",
    "\n",
    "    CONTEXTO:\n",
    "    {context}\n",
    "\n",
    "    PREGUNTA:\n",
    "    {question}\n",
    "\n",
    "    RESPUESTA BASADA ÚNICAMENTE EN EL CONTEXTO:\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 🔁 FUNCIÓN RAG\n",
    "\n",
    "def respuesta_rag(query: str) -> str:\n",
    "    contexto = \"\\n\\n\".join(retrieve(query))\n",
    "    prompt = prompt_template.format(context=contexto, question=query)\n",
    "    respuesta = llm.invoke(prompt)\n",
    "    return respuesta\n",
    "\n",
    "\n",
    "# ✅ CONSULTA MANUAL DESDE NOTEBOOK\n",
    "\n",
    "query = \"What are the side effects of tacrolimus\"\n",
    "\n",
    "\n",
    "if not query.strip():\n",
    "    print(\"❗ Pregunta vacía. Intenta escribir algo.\")\n",
    "else:\n",
    "    print(\"\\n📚 Pasajes recuperados (TOP_K):\")\n",
    "    for idx, txt in enumerate(retrieve(query), 1):\n",
    "        print(f\"\\n• Pasaje {idx}:\\n{textwrap.shorten(txt, 350)}\")\n",
    "\n",
    "    print(\"\\n🤖 Respuesta RAG (modelo local):\")\n",
    "    print(respuesta_rag(query))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LucasBrusasca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
